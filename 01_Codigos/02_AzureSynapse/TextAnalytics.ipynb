{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analísis de sentimientos"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AzureKeyCredential\r\n",
        "from azure.ai.textanalytics import (\r\n",
        "    TextAnalyticsClient,\r\n",
        "    RecognizeEntitiesAction,\r\n",
        "    AnalyzeSentimentAction,\r\n",
        ")\r\n",
        "\r\n",
        "credential = AzureKeyCredential(\"505c3ba3a57a4b749a5e5a96a589d39a\")\r\n",
        "endpoint=\"https://eastus2.api.cognitive.microsoft.com/\"\r\n",
        "\r\n",
        "text_analytics_client = TextAnalyticsClient(endpoint, credential)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"La universidad Distrital tiene más presupueto que la Universidad Javeriana.\",\r\n",
        "    \"La pandemia no solo trajo cosas malas, hemos aprendido mucho de ello\",\r\n",
        "    \"Odio la gente gomela en los restaurantes\",\r\n",
        "    'Mañana sadré a comer con mi novia',\r\n",
        "    'Hoy tengo clase de AI en la novhe, qué emoción',\r\n",
        "    'mi perro Tobias debe lavarse los dientes'\r\n",
        "]\r\n",
        "\r\n",
        "response = text_analytics_client.analyze_sentiment(documents, language=\"es\")\r\n",
        "result = [doc for doc in response if not doc.is_error]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in result:\r\n",
        "    print(\"Overall sentiment: {}\".format(doc.sentiment))\r\n",
        "    print(\"Scores: positive={}; neutral={}; negative={} \\n\".format(\r\n",
        "        doc.confidence_scores.positive,\r\n",
        "        doc.confidence_scores.neutral,\r\n",
        "        doc.confidence_scores.negative,\r\n",
        "    ))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocer entidades\r\n",
        "Recognition_entities reconoce y categoriza entidades en su texto de entrada como personas, lugares, organizaciones, fecha / hora, cantidades, porcentajes, monedas y más."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = text_analytics_client.recognize_entities(documents, language=\"es\")\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    for entity in doc.entities:\r\n",
        "        print(\"Entity: {}\".format(entity.text))\r\n",
        "        print(\"...Category: {}\".format(entity.category))\r\n",
        "        print(\"...Confidence Score: {}\".format(entity.confidence_score))\r\n",
        "        print(\"...Offset: {}\".format(entity.offset))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocer entidades vinculadas\r\n",
        "Recognition_linked_entities reconoce y elimina la ambigüedad de la identidad de cada entidad que se encuentra en su texto de entrada (por ejemplo, determina si una aparición de la palabra Marte se refiere al planeta o al dios romano de la guerra). Las entidades reconocidas se asocian con URL a una base de conocimientos conocida, como Wikipedia.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = text_analytics_client.recognize_linked_entities(documents, language=\"es\")\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    for entity in doc.entities:\r\n",
        "        print(\"Entity: {}\".format(entity.name))\r\n",
        "        print(\"...URL: {}\".format(entity.url))\r\n",
        "        print(\"...Data Source: {}\".format(entity.data_source))\r\n",
        "        print(\"...Entity matches:\")\r\n",
        "        for match in entity.matches:\r\n",
        "            print(\"......Entity match text: {}\".format(match.text))\r\n",
        "            print(\"......Confidence Score: {}\".format(match.confidence_score))\r\n",
        "            print(\"......Offset: {}\".format(match.offset))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocer entidades PII\r\n",
        "Recognition_pii_entities reconoce y categoriza entidades de información de identificación personal (PII) en su texto de entrada, como números de seguro social, información de cuentas bancarias, números de tarjetas de crédito y más."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = text_analytics_client.recognize_pii_entities(documents, language=\"es\")\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for idx, doc in enumerate(result):\r\n",
        "    print(\"Document text: {}\".format(documents[idx]))\r\n",
        "    print(\"Redacted document text: {}\".format(doc.redacted_text))\r\n",
        "    for entity in doc.entities:\r\n",
        "        print(\"...Entity: {}\".format(entity.text))\r\n",
        "        print(\"......Category: {}\".format(entity.category))\r\n",
        "        print(\"......Confidence Score: {}\".format(entity.confidence_score))\r\n",
        "        print(\"......Offset: {}\".format(entity.offset))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraer frases clave\r\n",
        "extract_key_phrases determina los principales puntos de conversación en su texto de entrada. Por ejemplo, para el texto de entrada \"La comida estaba deliciosa y había un personal maravilloso\", la API devuelve: \"comida\" y \"personal maravilloso\"."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = text_analytics_client.extract_key_phrases(documents, language=\"en\")\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    print(doc.key_phrases)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detectar idioma\r\n",
        "detect_language determina el idioma de su texto de entrada, incluida la puntuación de confianza del idioma predicho."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = text_analytics_client.detect_language(documents)\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    print(\"Language detected: {}\".format(doc.primary_language.name))\r\n",
        "    print(\"ISO6391 name: {}\".format(doc.primary_language.iso6391_name))\r\n",
        "    print(\"Confidence score: {}\\n\".format(doc.primary_language.confidence_score))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis multiple"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poller = text_analytics_client.begin_analyze_actions(\r\n",
        "    documents,\r\n",
        "    display_name=\"Sample Text Analysis\",\r\n",
        "    actions=[\r\n",
        "        RecognizeEntitiesAction(),\r\n",
        "        AnalyzeSentimentAction()\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "# returns multiple actions results in the same order as the inputted actions\r\n",
        "document_results = poller.result()\r\n",
        "for doc, action_results in zip(documents, document_results):\r\n",
        "    recognize_entities_result, analyze_sentiment_result = action_results\r\n",
        "    print(\"\\nDocument text: {}\".format(doc))\r\n",
        "    print(\"...Results of Recognize Entities Action:\")\r\n",
        "    if recognize_entities_result.is_error:\r\n",
        "        print(\"......Is an error with code '{}' and message '{}'\".format(\r\n",
        "            recognize_entities_result.code, recognize_entities_result.message\r\n",
        "        ))\r\n",
        "    else:\r\n",
        "        for entity in recognize_entities_result.entities:\r\n",
        "            print(\"......Entity: {}\".format(entity.text))\r\n",
        "            print(\".........Category: {}\".format(entity.category))\r\n",
        "            print(\".........Confidence Score: {}\".format(entity.confidence_score))\r\n",
        "            print(\".........Offset: {}\".format(entity.offset))\r\n",
        "\r\n",
        "    print(\"...Results of Analyze Sentiment action:\")\r\n",
        "    if analyze_sentiment_result.is_error:\r\n",
        "        print(\"......Is an error with code '{}' and message '{}'\".format(\r\n",
        "            analyze_sentiment_result.code, analyze_sentiment_result.message\r\n",
        "        ))\r\n",
        "    else:\r\n",
        "        print(\"......Overall sentiment: {}\".format(analyze_sentiment_result.sentiment))\r\n",
        "        print(\"......Scores: positive={}; neutral={}; negative={} \\n\".format(\r\n",
        "            analyze_sentiment_result.confidence_scores.positive,\r\n",
        "            analyze_sentiment_result.confidence_scores.neutral,\r\n",
        "            analyze_sentiment_result.confidence_scores.negative,\r\n",
        "        ))\r\n",
        "    print(\"------------------------------------------\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}